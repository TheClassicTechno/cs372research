{# ===========================================================================
   RAudit CRIT User Prompt — Reasoning Trace to Evaluate
   ===========================================================================

   This template is filled with the specific reasoning trace being evaluated.
   The variables come from a CanonicalTrace (via the TranscriptParser):

     claim            = The agent's conclusion Omega. In CRIT terminology, this
                        is the proposition being defended. The four pillars all
                        evaluate whether the reasoning supports THIS claim.

     reasons          = Supporting reasons R. These are the arguments and
                        evidence the agent marshalled in favour of the claim.
                        Includes justification text + individual claims with
                        Pearl causal levels (L1/L2/L3) and confidence scores.

     counterarguments = Rival reasons R'. Objections, risks, and alternative
                        hypotheses that the agent considered. Strong reasoning
                        engages seriously with R' — Pillar 3 checks this.
                        If "None identified.", Pillar 3 will score low.

     assumptions      = Key assumptions underlying the reasoning, extracted
                        from the structured claims. Making assumptions explicit
                        is a sign of good reasoning; hidden assumptions are a
                        reasoning pathology.

     final_decision   = The concrete action taken (e.g., "BUY 150 AAPL").
                        Checked for trace-output consistency — does the
                        decision match what the reasoning actually derives?

     context          = Optional market/task context from the debate trace's
                        what_i_saw field. Provides background WITHOUT revealing
                        ground truth (maintains the blindness constraint).

   NOTATION:
     Omega (Ω) = the claim being evaluated
     R         = supporting reasons (arguments for Omega)
     R'        = rival reasons (arguments against Omega)
     gamma (γ) = mean of four pillar scores (reasonableness)
     theta (θ) = structural confidence (calibration quality)
     ρ (rho)   = gamma (the composite reasonableness dial output)

   The evaluator LLM returns a JSON object with per-pillar scores (1-10),
   trace-output consistency (bool), gamma (1-10), theta (1-10), and notes.
   The scorer then normalises 1-10 to [0,1] by dividing by 10.
#}
Evaluate the following reasoning trace using the RAudit CRIT reasonableness dial.

Remember: You are blind to ground truth. Evaluate ONLY whether the derivation supports the conclusion.

{% if context %}
CONTEXT (market/task setting):
{{ context }}
{% endif %}

REASONING TRACE TO EVALUATE:
---
CLAIM (Ω): {{ claim }}

SUPPORTING REASONS (R):
{{ reasons }}

COUNTERARGUMENTS CONSIDERED (R'):
{{ counterarguments }}

KEY ASSUMPTIONS:
{{ assumptions }}

FINAL DECISION:
{{ final_decision }}
---

Score each of the four pillars (1-10) and provide justifications. Then compute:
- gamma (γ): mean of the four pillar scores (1-10)
- theta (θ): structural confidence score (1-10) — how well-calibrated is the overall argument structure? Consider whether confidence levels match evidence strength, whether uncertainty is appropriately acknowledged, and whether the trace-output is consistent.

Respond with a JSON object in this exact format:
{
  "pillars": {
    "logical_validity": {
      "score": <1-10>,
      "justification": "<specific evidence from trace>"
    },
    "evidential_support": {
      "score": <1-10>,
      "justification": "<specific evidence from trace>"
    },
    "alternative_consideration": {
      "score": <1-10>,
      "justification": "<specific evidence from trace>"
    },
    "causal_alignment": {
      "score": <1-10>,
      "justification": "<specific evidence from trace>"
    }
  },
  "trace_output_consistent": <true|false>,
  "gamma": <1-10>,
  "theta": <1-10>,
  "notes": "<overall blind assessment of reasoning quality>"
}